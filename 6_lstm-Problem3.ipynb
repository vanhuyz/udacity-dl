{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: 誰\n",
      "1 26 0 None\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return \n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('誰'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://github.com/vanhuyz/udacity-dl/blob/master/6_lstm-Problem2.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(string.ascii_lowercase[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: 誰\n",
      "3 28 0 None 1 2\n",
      "<go> x  \n",
      "[22, 10, 7, 0, 19, 23, 11, 5, 13, 0, 4, 20, 17, 25, 16, 0, 8, 17, 26]\n",
      "[1, 7, 10, 22, 0, 13, 5, 11, 23, 19, 0, 16, 25, 17, 20, 4, 0, 26, 17, 8]\n",
      "[7, 10, 22, 0, 13, 5, 11, 23, 19, 0, 16, 25, 17, 20, 4, 0, 26, 17, 8, 2]\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 3 # [a-z] + ' ' + <go> + <eos>\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "GO_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 3\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  elif char == '<go>':\n",
    "    return 1\n",
    "  elif char == '<eos>':\n",
    "    return 2\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return \n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 2:\n",
    "    return chr(dictid + first_letter - 3)\n",
    "  elif dictid == GO_ID:\n",
    "    return '<go>'\n",
    "  elif dictid == EOS_ID:\n",
    "    return '<eos>'\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('誰'), char2id('<go>'), char2id('<eos>'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n",
    "\n",
    "def str2id(str):\n",
    "  return [char2id(c) for c in str]\n",
    "\n",
    "print(str2id('the quick brown fox'))\n",
    "print(str2id('eht kciuq nworb xof'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sno ihcrana\n"
     ]
    }
   ],
   "source": [
    "def reverse_word(word):\n",
    "  return word[::-1]\n",
    "\n",
    "def mirror_sequence(seq):\n",
    "  return ' '.join(list(map(lambda word: reverse_word(word), seq.split(' '))))\n",
    "\n",
    "print(mirror_sequence('ons anarchi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advoc', 'when military govern', 'lleria arches nation', ' abbeys and monaster', 'married urraca princ', 'hel and richard baer', 'y and liturgical lan', 'ay opened for passen', 'tion from the nation', 'migration took place', 'new york other well ', 'he boeing seven six ', 'e listed with a glos', 'eber has probably be', 'o be made to recogni', 'yer who received the', 'ore significant than', 'a fierce critic of t', ' two six eight in si', 'aristotle s uncaused', 'ity can be lost as i', ' and intracellular i', 'tion of the size of ', 'dy to pass him a sti', 'f certain drugs conf', 'at it will take to c', 'e convince the pries', 'ent told him to name', 'ampaign and barred a', 'rver side standard f', 'ious texts such as e', 'o capitalize on the ', 'a duplicate of the o', 'gh ann es d hiver on', 'ine january eight ma', 'ross zero the lead c', 'cal theories classic', 'ast instance the non', ' dimensional analysi', 'most holy mormons be', 't s support or at le', 'u is still disagreed', 'e oscillating system', 'o eight subtypes bas', 'of italy languages t', 's the tower commissi', 'klahoma press one ni', 'erprise linux suse l', 'ws becomes the first', 'et in a nazi concent', 'the fabian society n', 'etchy to relatively ', ' sharman networks sh', 'ised emperor hirohit', 'ting in political in', 'd neo latin most of ', 'th risky riskerdoo r', 'encyclopedic overvie', 'fense the air compon', 'duating from acnm ac', 'treet grid centerlin', 'ations more than any', 'appeal of devotional', 'si have made such de']\n",
      "['sno stsihcrana covda', 'nehw yratilim nrevog', 'airell sehcra noitan', ' syebba dna retsanom', 'deirram acarru cnirp', 'leh dna drahcir reab', 'y dna lacigrutil nal', 'ya denepo rof nessap', 'noit morf eht noitan', 'noitargim koot ecalp', 'wen kroy rehto llew ', 'eh gnieob neves xis ', 'e detsil htiw a solg', 'rebe sah ylbaborp eb', 'o eb edam ot ingocer', 'rey ohw deviecer eht', 'ero tnacifingis naht', 'a ecreif citirc fo t', ' owt xis thgie ni is', 'eltotsira s desuacnu', 'yti nac eb tsol sa i', ' dna ralullecartni i', 'noit fo eht ezis fo ', 'yd ot ssap mih a its', 'f niatrec sgurd fnoc', 'ta ti lliw ekat ot c', 'e ecnivnoc eht seirp', 'tne dlot mih ot eman', 'ngiapma dna derrab a', 'revr edis dradnats f', 'suoi stxet hcus sa e', 'o ezilatipac no eht ', 'a etacilpud fo eht o', 'hg nna se d revih no', 'eni yraunaj thgie am', 'ssor orez eht dael c', 'lac seiroeht cissalc', 'tsa ecnatsni eht non', ' lanoisnemid isylana', 'tsom yloh snomrom eb', 't s troppus ro ta el', 'u si llits deergasid', 'e gnitallicso metsys', 'o thgie sepytbus sab', 'fo ylati segaugnal t', 's eht rewot issimmoc', 'amohalk sserp eno in', 'esirpre xunil esus l', 'sw semoceb eht tsrif', 'te ni a izan tnecnoc', 'eht naibaf yteicos n', 'yhcte ot ylevitaler ', ' namrahs skrowten hs', 'desi rorepme tihorih', 'gnit ni lacitilop ni', 'd oen nital tsom fo ', 'ht yksir oodreksir r', 'cidepolcycne eivrevo', 'esnef eht ria nopmoc', 'gnitaud morf mnca ca', 'teert dirg nilretnec', 'snoita erom naht yna', 'laeppa fo lanoitoved', 'is evah edam hcus ed']\n",
      "[' anarchism']\n",
      "['m originat']\n",
      "['ted as a t']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "sequence_length = 20\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, sequence_length):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._sequence_length = sequence_length\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data.\n",
    "    \"\"\"\n",
    "    batches = [None] * self._batch_size\n",
    "    for i in range(self._batch_size):\n",
    "      batches[i] = self._text[self._cursor[i]:self._cursor[i]+self._sequence_length]\n",
    "      self._cursor[i] = (self._cursor[i] + self._sequence_length - 1) % self._text_size\n",
    "    return batches\n",
    "\n",
    "  #def next(self):\n",
    "  #  return [str2id(batch) for batch in self.next_sequence()]\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def mirror_batches(batches):\n",
    "   return [mirror_sequence(seq) for seq in batches]\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, sequence_length)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 10)\n",
    "\n",
    "batches = train_batches.next()\n",
    "print(batches)\n",
    "print(mirror_batches(batches))\n",
    "print(valid_batches.next())\n",
    "print(valid_batches.next())\n",
    "print(valid_batches.next())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of weight: 21\n",
      "shape of weight[0]: (?,)\n",
      "21\n",
      "(?, 29)\n",
      "(21, ?, 29)\n"
     ]
    }
   ],
   "source": [
    "lstm_size = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  encoder_inputs = list()\n",
    "  decoder_inputs = list()\n",
    "  train_labels = list()  \n",
    "\n",
    "  for _ in range(seq_size):\n",
    "    encoder_inputs.append(tf.placeholder(tf.int32, shape=(None,)))\n",
    "  for _ in range(seq_size+1):\n",
    "    decoder_inputs.append(tf.placeholder(tf.int32, shape=(None,)))\n",
    "    train_labels.append(tf.placeholder(tf.int32, shape=(None,)))\n",
    "\n",
    "  weights = [tf.ones_like(labels_t, dtype=tf.float32) for labels_t in train_labels]\n",
    "\n",
    "  print('length of weight:', len(weights))\n",
    "  print('shape of weight[0]:', weights[0].get_shape())\n",
    "\n",
    "  # Use LSTM cell\n",
    "  cell = tf.nn.rnn_cell.BasicLSTMCell(lstm_size)\n",
    "  #outputs, states = tf.nn.seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "  outputs, states = tf.nn.seq2seq.embedding_rnn_seq2seq(encoder_inputs,\n",
    "                                                        decoder_inputs,\n",
    "                                                        cell,\n",
    "                                                        vocabulary_size, # num_encoder_symbols\n",
    "                                                        vocabulary_size, # num_decoder_symbols\n",
    "                                                        vocabulary_size, # embedding_size\n",
    "                                                      )\n",
    "  \n",
    "  print(len(outputs))\n",
    "  print(outputs[0].get_shape())\n",
    "\n",
    "  loss = tf.nn.seq2seq.sequence_loss(outputs, train_labels, weights) \n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.1).minimize(loss)\n",
    "\n",
    "  # Predictions.\n",
    "  train_predictions = tf.pack([tf.nn.softmax(output) for output in outputs])\n",
    "  print(train_predictions.get_shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 3.382061 \n",
      "Input            :  neity and chance are\n",
      "Correct Output   :  ytien dna ecnahc era\n",
      "Generated Output :  <go><go>vttvvvvvdfdcdcjssss\n",
      "Loss at step 1000: 2.671503 \n",
      "Input            :  nine to present best\n",
      "Correct Output   :  enin ot tneserp tseb\n",
      "Generated Output :  e    e  ee      ee<eos><eos><eos>\n",
      "Loss at step 2000: 2.433356 \n",
      "Input            :  ndamentally selfish \n",
      "Correct Output   :  yllatnemadn hsifles \n",
      "Generated Output :     a  a     et      <eos>\n",
      "Loss at step 3000: 2.342326 \n",
      "Input            :   universal studios r\n",
      "Correct Output   :   lasrevinu soiduts r\n",
      "Generated Output :  eeenaeta   si      <eos><eos>\n",
      "Loss at step 4000: 2.088270 \n",
      "Input            :  ht edited by leonard\n",
      "Correct Output   :  th detide yb dranoel\n",
      "Generated Output :  t teehnr rsn nnererr<eos>\n",
      "Loss at step 5000: 1.950407 \n",
      "Input            :   spain statistics re\n",
      "Correct Output   :   niaps scitsitats er\n",
      "Generated Output :   eitt  stitset   ee<eos><eos>\n",
      "Loss at step 6000: 1.790616 \n",
      "Input            :  ng the theme of a co\n",
      "Correct Output   :  gn eht emeht fo a oc\n",
      "Generated Output :  teeeht ehoh  oo o ff<eos>\n",
      "Loss at step 7000: 1.656677 \n",
      "Input            :  ally beneficial arra\n",
      "Correct Output   :  ylla laicifeneb arra\n",
      "Generated Output :  llaa nanrnr     rraa<eos>\n",
      "Loss at step 8000: 1.536671 \n",
      "Input            :  onship with francisc\n",
      "Correct Output   :  pihsno htiw csicnarf\n",
      "Generated Output :  siss   st s dnarrarp<eos>\n",
      "Loss at step 9000: 1.442895 \n",
      "Input            :  rden s anniversary p\n",
      "Correct Output   :  nedr s yrasrevinna p\n",
      "Generated Output :  ded  sinninnenini  f<eos>\n",
      "Loss at step 10000: 1.297288 \n",
      "Input            :  ometimes valuable bu\n",
      "Correct Output   :  semitemo elbaulav ub\n",
      "Generated Output :  setita   llaallac ed<eos>\n",
      "Loss at step 11000: 1.268063 \n",
      "Input            :  ssible we are also i\n",
      "Correct Output   :  elbiss ew era osla i\n",
      "Generated Output :  ssssss emoamammmaa i<eos>\n",
      "Loss at step 12000: 1.148009 \n",
      "Input            :  the field was still \n",
      "Correct Output   :  eht dleif saw llits \n",
      "Generated Output :  eht eeehf s   liits <eos>\n",
      "Loss at step 13000: 1.094804 \n",
      "Input            :  onomy more than in a\n",
      "Correct Output   :  ymono erom naht ni a\n",
      "Generated Output :  ooono eron na   ii a<eos>\n",
      "Loss at step 14000: 0.945881 \n",
      "Input            :  stood to form the ba\n",
      "Correct Output   :  doots ot mrof eht ab\n",
      "Generated Output :  sott  ot rre  eht an<eos>\n",
      "Loss at step 15000: 0.935338 \n",
      "Input            :  ny and were often pe\n",
      "Correct Output   :  yn dna erew netfo ep\n",
      "Generated Output :  dn dna eref net o e<eos><eos>\n",
      "Loss at step 16000: 0.930594 \n",
      "Input            :  e full capability of\n",
      "Correct Output   :  e lluf ytilibapac fo\n",
      "Generated Output :  e llaa ytalaaatat fo<eos>\n",
      "Loss at step 17000: 1.011283 \n",
      "Input            :   strong in austria a\n",
      "Correct Output   :   gnorts ni airtsua a\n",
      "Generated Output :   nnort  ni airaaaa a<eos>\n",
      "Loss at step 18000: 0.906886 \n",
      "Input            :   two nine separate c\n",
      "Correct Output   :   owt enin etarapes c\n",
      "Generated Output :   owt enit etareess <eos><eos>\n",
      "Loss at step 19000: 0.775001 \n",
      "Input            :  na following the abo\n",
      "Correct Output   :  an gniwollof eht oba\n",
      "Generated Output :  nn gnololfof eht oba<eos>\n",
      "Loss at step 20000: 0.795310 \n",
      "Input            :  ns in american samoa\n",
      "Correct Output   :  sn ni nacirema aomas\n",
      "Generated Output :  nn ni nacarama ammas<eos>\n",
      "Loss at step 21000: 0.708633 \n",
      "Input            :  even cp three six se\n",
      "Correct Output   :  neve pc eerht xis es\n",
      "Generated Output :  neve e  eehht sis es<eos>\n",
      "Loss at step 22000: 0.738749 \n",
      "Input            :   hiv and the associa\n",
      "Correct Output   :   vih dna eht aicossa\n",
      "Generated Output :   nih dna eht amssssa<eos>\n",
      "Loss at step 23000: 0.675797 \n",
      "Input            :  movement an irish an\n",
      "Correct Output   :  tnemevom na hsiri na\n",
      "Generated Output :  tnememom aa stir  na<eos>\n",
      "Loss at step 24000: 0.673943 \n",
      "Input            :  n tradition or of th\n",
      "Correct Output   :  n noitidart ro fo ht\n",
      "Generated Output :  n noitarart foffo ht<eos>\n",
      "Loss at step 25000: 0.648455 \n",
      "Input            :   spectators on each \n",
      "Correct Output   :   srotatceps no hcae \n",
      "Generated Output :   srotttceps no hcagc<eos>\n",
      "Loss at step 26000: 0.639845 \n",
      "Input            :  ne two defeated gora\n",
      "Correct Output   :  en owt detaefed arog\n",
      "Generated Output :  en ott detaefed arog<eos>\n",
      "Loss at step 27000: 0.520970 \n",
      "Input            :  ne nine zero est fre\n",
      "Correct Output   :  en enin orez tse erf\n",
      "Generated Output :  en enin orez sse rrf<eos>\n",
      "Loss at step 28000: 0.628411 \n",
      "Input            :  in al gore a user s \n",
      "Correct Output   :  ni la erog a resu s \n",
      "Generated Output :  ni aa ero  e ses  s <eos>\n",
      "Loss at step 29000: 0.536434 \n",
      "Input            :  n another episode af\n",
      "Correct Output   :  n rehtona edosipe fa\n",
      "Generated Output :  n rehtona esosie  fo<eos>\n",
      "Loss at step 30000: 0.587409 \n",
      "Input            :  r two zero zero two \n",
      "Correct Output   :  r owt orez orez owt \n",
      "Generated Output :  r owt orez orez owt <eos>\n",
      "Loss at step 31000: 0.497486 \n",
      "Input            :  guards inspired by c\n",
      "Correct Output   :  sdraug deripsni yb c\n",
      "Generated Output :  srrarg derissni yb c<eos>\n",
      "Loss at step 32000: 0.532908 \n",
      "Input            :  rge w bush won the s\n",
      "Correct Output   :  egr w hsub now eht s\n",
      "Generated Output :  evr u hcub no  eht s<eos>\n",
      "Loss at step 33000: 0.427610 \n",
      "Input            :  rnor during the buil\n",
      "Correct Output   :  ronr gnirud eht liub\n",
      "Generated Output :  ronr gnirud aht libb<eos>\n",
      "Loss at step 34000: 0.452017 \n",
      "Input            :  increased from aroun\n",
      "Correct Output   :  desaercni morf nuora\n",
      "Generated Output :  decarccni morf noora<eos>\n",
      "Loss at step 35000: 0.489577 \n",
      "Input            :  e one nine two six b\n",
      "Correct Output   :  e eno enin owt xis b\n",
      "Generated Output :  e eno enin owt ci  b<eos>\n",
      "Loss at step 36000: 0.478529 \n",
      "Input            :  resent the fundament\n",
      "Correct Output   :  tneser eht tnemadnuf\n",
      "Generated Output :  tneeer ebt nnemdlnuf<eos>\n",
      "Loss at step 37000: 0.431691 \n",
      "Input            :  anes with an odd num\n",
      "Correct Output   :  sena htiw na ddo mun\n",
      "Generated Output :  sena htiw na dnnwnuh<eos>\n",
      "Loss at step 38000: 0.447233 \n",
      "Input            :  he evidence and reve\n",
      "Correct Output   :  eh ecnedive dna ever\n",
      "Generated Output :  eh ecnedive dna ever<eos>\n",
      "Loss at step 39000: 0.416048 \n",
      "Input            :   assistive technolog\n",
      "Correct Output   :   evitsissa golonhcet\n",
      "Generated Output :   esissiso  gnlbnhcet<eos>\n",
      "Loss at step 40000: 0.405804 \n",
      "Input            :  he molecular orbital\n",
      "Correct Output   :  eh ralucelom latibro\n",
      "Generated Output :  eh lelumelom latibro<eos>\n",
      "Loss at step 41000: 0.416678 \n",
      "Input            :   first television st\n",
      "Correct Output   :   tsrif noisivelet ts\n",
      "Generated Output :   tsrif noiseeelet ts<eos>\n",
      "Loss at step 42000: 0.362888 \n",
      "Input            :  tive cultures the al\n",
      "Correct Output   :  evit serutluc eht la\n",
      "Generated Output :  evis serutluc eht la<eos>\n",
      "Loss at step 43000: 0.432085 \n",
      "Input            :  ind the last line of\n",
      "Correct Output   :  dni eht tsal enil fo\n",
      "Generated Output :  dni eht tsae enil oo<eos>\n",
      "Loss at step 44000: 0.370264 \n",
      "Input            :  e same time the crew\n",
      "Correct Output   :  e emas emit eht werc\n",
      "Generated Output :  e emas emit eht rerc<eos>\n",
      "Loss at step 45000: 0.350635 \n",
      "Input            :  orbit around the moo\n",
      "Correct Output   :  tibro dnuora eht oom\n",
      "Generated Output :  tibro dnuora eht oom<eos>\n",
      "Loss at step 46000: 0.374654 \n",
      "Input            :  balistic proposal an\n",
      "Correct Output   :  citsilab lasoporp na\n",
      "Generated Output :  citiilah yamoporp na<eos>\n",
      "Loss at step 47000: 0.365362 \n",
      "Input            :  hree moseley discove\n",
      "Correct Output   :  eerh yelesom evocsid\n",
      "Generated Output :  errh yememom evocsid<eos>\n",
      "Loss at step 48000: 0.394172 \n",
      "Input            :   related to them cal\n",
      "Correct Output   :   detaler ot meht lac\n",
      "Generated Output :   detaler ot eeht lac<eos>\n",
      "Loss at step 49000: 0.341590 \n",
      "Input            :  iflora agave geminif\n",
      "Correct Output   :  arolfi evaga finimeg\n",
      "Generated Output :  arolfi evaha ninimeg<eos>\n",
      "Loss at step 50000: 0.335470 \n",
      "Input            :  r employers in india\n",
      "Correct Output   :  r sreyolpme ni aidni\n",
      "Generated Output :  r sreyolpme ni aidni<eos>\n"
     ]
    }
   ],
   "source": [
    "def first_prediction(predictions):\n",
    "  return ''.join([characters(onehot)[0] for onehot in predictions])\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "  sess.run(tf.initialize_all_variables())\n",
    "\n",
    "  for step in range(50001):\n",
    "    feed_dict = dict()\n",
    "    \n",
    "    input_batches = train_batches.next()\n",
    "    output_batches = mirror_batches(input_batches)\n",
    "       \n",
    "    feed_dict[decoder_inputs[0]] = [GO_ID] * batch_size\n",
    "    for i in range(sequence_length):\n",
    "      feed_dict[encoder_inputs[i]] = [char2id(seq[i]) for seq in input_batches]\n",
    "      feed_dict[decoder_inputs[i+1]] = [char2id(seq[i]) for seq in output_batches]\n",
    "      feed_dict[train_labels[i]] = [char2id(seq[i]) for seq in output_batches]\n",
    "    feed_dict[train_labels[sequence_length]] = [EOS_ID] * batch_size\n",
    "    \n",
    "    _, l, predictions = sess.run([optimizer, loss, train_predictions], feed_dict=feed_dict)\n",
    "\n",
    "    if step % 1000 == 0:\n",
    "      print('Loss at step %d: %f ' % (step,l))\n",
    "      print('Input            : ', input_batches[0])\n",
    "      print('Correct Output   : ', output_batches[0])\n",
    "      print('Generated Output : ', first_prediction(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def final_prediction(b):\n",
    "  return ''.join([characters(onehot)[b] for onehot in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spontaneity spontan\n",
      " ytienatnops natnops\n",
      " nnnnnonnoc  notnocs<eos>\n",
      "==========\n",
      "nder general howe se\n",
      "redn lareneg ewoh es\n",
      "seln eaneveh eho  e<eos><eos>\n",
      "==========\n",
      "der homicide rape tr\n",
      "red edicimoh epar rt\n",
      "eed ehitihorterarttt<eos>\n",
      "==========\n",
      " instruction does so\n",
      " noitcurtsni seod os\n",
      " toitiitisii eerf f<eos><eos>\n",
      "==========\n",
      "cupid song cupid as \n",
      "dipuc gnos dipuc sa \n",
      "dilu  diii dilac da <eos>\n",
      "==========\n",
      "ph of lecture ix to \n",
      "hp fo erutcel xi ot \n",
      "t  eo eteteed ti ft <eos>\n",
      "==========\n",
      "teen game schedule d\n",
      "neet emag eludehcs d\n",
      "eeet edededdddesc  d<eos>\n",
      "==========\n",
      "d by martin brennan \n",
      "d yb nitram nannerb \n",
      "d nn naraannneneerp <eos>\n",
      "==========\n",
      "en concrete steps to\n",
      "ne etercnoc spets ot\n",
      "n  etrrtiop tett  tt<eos>\n",
      "==========\n",
      "west the doya trace \n",
      "tsew eht ayod ecart \n",
      "eeeh eht artteerart <eos>\n",
      "==========\n",
      "erance and grandeur \n",
      "ecnare dna ruednarg \n",
      "eraare sna rellnarp<eos><eos>\n",
      "==========\n",
      "onsider the position\n",
      "redisno eht noitisop\n",
      "eehnnno eht tiitirod<eos>\n",
      "==========\n",
      "trait of building da\n",
      "tiart fo gnidliub ad\n",
      "tatt  oo lnillall dd<eos>\n",
      "==========\n",
      " coefficient bounds \n",
      " tneiciffeoc sdnuob \n",
      " eneitefoedc dcnooc<eos><eos>\n",
      "==========\n",
      "n a simple confidenc\n",
      "n a elpmis cnedifnoc\n",
      "  e elliis nnetnnooc<eos>\n",
      "==========\n",
      " see also the unix p\n",
      " ees osla eht xinu p\n",
      " ess ssaa eht iin  e<eos>\n",
      "==========\n",
      "ountry or region in \n",
      "yrtnuo ro noiger ni \n",
      "ntonoo no niieevini <eos>\n",
      "==========\n",
      "ords to mention a po\n",
      "sdro ot noitnem a op\n",
      "dtn  ttaniitaem a tt<eos>\n",
      "==========\n",
      "expositions about sp\n",
      "snoitisopxe tuoba ps\n",
      "nnoititoceeptall ld<eos><eos>\n",
      "==========\n",
      "lth english the term\n",
      "htl hsilgne eht mret\n",
      "sgs sgineee eht teet<eos>\n",
      "==========\n",
      "s and a rotation spe\n",
      "s dna a noitator eps\n",
      "s dna a tiitatrrees<eos><eos>\n",
      "==========\n",
      " upon seeing this vi\n",
      " nopu gniees siht iv\n",
      " noie eeeess sitt i<eos><eos>\n",
      "==========\n",
      "ut speed since speed\n",
      "tu deeps ecnis deeps\n",
      "d  devds ddiideeeed<eos><eos>\n",
      "==========\n",
      "s of individuals in \n",
      "s fo slaudividni ni \n",
      "  di diillililni di <eos>\n",
      "==========\n",
      " flow from the p typ\n",
      " wolf morf eht p pyt\n",
      " fof  oo   eht ttttt<eos>\n",
      "==========\n",
      "re of work while rec\n",
      "er fo krow elihw cer\n",
      "rr oo eoof ehiht eed<eos>\n",
      "==========\n",
      "ound the police had \n",
      "dnuo eht ecilop dah \n",
      "deoo eht elilap aa  <eos>\n",
      "==========\n",
      "hesis of mendelian g\n",
      "siseh fo nailednem g\n",
      "sese  eo eeeleleed <eos><eos>\n",
      "==========\n",
      "rope early life thom\n",
      "epor ylrae efil moht\n",
      "era  eeeeepetitooott<eos>\n",
      "==========\n",
      "ten a difficult unde\n",
      "net a tluciffid ednu\n",
      "ee  d dnilidiid edne<eos>\n",
      "==========\n",
      "collected during the\n",
      "detcelloc gnirud eht\n",
      "dellelllc dnidtteett<eos>\n",
      "==========\n",
      "s the pitch must be \n",
      "s eht hctip tsum eb \n",
      "  eht ttiid dsa  ev <eos>\n",
      "==========\n",
      "tion of the effect o\n",
      "noit fo eht tceffe o\n",
      "toit eo ett eeeeee <eos><eos>\n",
      "==========\n",
      "is was largely becau\n",
      "si saw ylegral uaceb\n",
      "si sa  elelaaleeeeee<eos>\n",
      "==========\n",
      " converting juice in\n",
      " gnitrevnoc eciuj ni\n",
      " tnitrecnoc enin  ni<eos>\n",
      "==========\n",
      "and was virtually ab\n",
      "dna saw yllautriv ba\n",
      "sna sa  llaattaal aa<eos>\n",
      "==========\n",
      "harles britain began\n",
      "selrah niatirb nageb\n",
      "seraahttitticc neced<eos>\n",
      "==========\n",
      "he had a daughter ju\n",
      "eh dah a rethguad uj\n",
      "eh da  a depacapd ap<eos>\n",
      "==========\n",
      "wrote book one one o\n",
      "etorw koob eno eno o\n",
      "erore eoe  ene eno t<eos>\n",
      "==========\n",
      " prussia as well geo\n",
      " aissurp sa llew oeg\n",
      " ssssala ea eeeh ett<eos>\n",
      "==========\n",
      "ving him much of the\n",
      "gniv mih hcum fo eht\n",
      "nni  nih dtuo eo eht<eos>\n",
      "==========\n",
      "al commentators pres\n",
      "la srotatnemmoc serp\n",
      "la taotameempop eerp<eos>\n",
      "==========\n",
      "ief only some specie\n",
      "fei ylno emos eiceps\n",
      "eon nnoo ehic ecced<eos><eos>\n",
      "==========\n",
      "esent not only preci\n",
      "tnese ton ylno icerp\n",
      "nnev  nnn nlla eler<eos><eos>\n",
      "==========\n",
      "entirely fictional t\n",
      "yleritne lanoitcif t\n",
      "eeetttne tittttiit t<eos>\n",
      "==========\n",
      "he deep revenge of t\n",
      "eh peed egnever fo t\n",
      "e  eedd ennerer tott<eos>\n",
      "==========\n",
      " of israel such idea\n",
      " fo learsi hcus aedi\n",
      " eo eelluicscie eed<eos><eos>\n",
      "==========\n",
      "ty purposes it is co\n",
      "yt sesoprup ti si oc\n",
      "dt seridiip di si oc<eos>\n",
      "==========\n",
      "lish politician d on\n",
      "hsil naicitilop d no\n",
      "st   tiititidof fooo<eos>\n",
      "==========\n",
      "een one of only two \n",
      "nee eno fo ylno owt \n",
      "eee nno fo dloo ttt <eos>\n",
      "==========\n",
      "that they could subs\n",
      "taht yeht dluoc sbus\n",
      "tott eett dlacc suas<eos>\n",
      "==========\n",
      "levation extremes lo\n",
      "noitavel semertxe ol\n",
      "noitiled detettee ot<eos>\n",
      "==========\n",
      "ator and strategist \n",
      "rota dna tsigetarts \n",
      "dat  daa tsetetrrts<eos><eos>\n",
      "==========\n",
      " three hops so the t\n",
      " eerht spoh os eht t\n",
      " ehhht shoh et eht t<eos>\n",
      "==========\n",
      "zero two est airport\n",
      "orez owt tse tropria\n",
      "ere  ott taettracaac<eos>\n",
      "==========\n",
      "n the los angeles te\n",
      "n eht sol selegna et\n",
      "n eht seleeeleteeeet<eos>\n",
      "==========\n",
      "efurbished in the ar\n",
      "dehsibrufe ni eht ra\n",
      "sescirree  ei eht da<eos>\n",
      "==========\n",
      "acbeth in a vision i\n",
      "htebca ni a noisiv i\n",
      "ttaca  ni iiiiiti  i<eos>\n",
      "==========\n",
      "ero zero eight five \n",
      "ore orez thgie evif \n",
      "or  eteh eetie eni  <eos>\n",
      "==========\n",
      "majority bamar ethni\n",
      "ytirojam ramab inhte\n",
      "ylaaamam aamii itet<eos><eos>\n",
      "==========\n",
      "ed castle stalker as\n",
      "de eltsac reklats sa\n",
      "de dlaaal eelaass aa<eos>\n",
      "==========\n",
      " by merging pp one f\n",
      " yb gnigrem pp eno f\n",
      " dl nnileed eo eno <eos><eos>\n",
      "==========\n",
      " ft and attempted ac\n",
      " tf dna detpmetta ca\n",
      " ts dea eetaaetaalaa<eos>\n",
      "==========\n",
      "vd copying by alteri\n",
      "dv gniypoc yb iretla\n",
      "dn gnimpo  dl ttataa<eos>\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "for i in range(batch_size):\n",
    "  print(input_batches[i])\n",
    "  print(output_batches[i])\n",
    "  print(final_prediction(i))\n",
    "  print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
